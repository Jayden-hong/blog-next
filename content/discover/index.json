{
  "date": "2026-02-17",
  "processed": [
    {
      "slug": "2025-llm-year-in-review-95d29eeb",
      "title": "2025 LLM Year in Review",
      "source": "karpathy-bear",
      "category": "General",
      "summary": "2025 LLM Year in Review 19 Dec, 2025 2025 has been a strong and eventful year of progress in LLMs. The following is a list of personally notable and mildly surprising \"paradigm changes\" - things that altered the landscape and stood out to me conceptually. Reinforcement Learning from Verifiable Rewards (RLVR)At the start of 2025, the LLM production stack in all labs looked something like this: Pretraining (GPT-2/3 of ~2020) Supervised Finetuning (InstructGPT ~2022) and Reinforcement Learning from Human Feedback (RLHF ~2022) This was the stable and proven recipe for training a production-grade LLM for a while.",
      "content": "2025 LLM Year in Review 19 Dec, 2025 2025 has been a strong and eventful year of progress in LLMs. The following is a list of personally notable and mildly surprising \"paradigm changes\" - things that altered the landscape and stood out to me conceptually. 1. Reinforcement Learning from Verifiable Rewards (RLVR)At the start of 2025, the LLM production stack in all labs looked something like this: Pretraining (GPT-2/3 of ~2020) Supervised Finetuning (InstructGPT ~2022) and Reinforcement Learning from Human Feedback (RLHF ~2022) This was the stable and proven recipe for training a production-grade LLM for a while. In 2025, Reinforcement Learning from Verifiable Rewards (RLVR) emerged as the de facto new major stage to add to this mix. By training LLMs against automatically verifiable rewards across a number of environments (e.g. think math/code puzzles), the LLMs spontaneously develop strategies that look like \"reasoning\" to humans - they learn to break down problem solving into intermediate calculations and they learn a number of problem solving strategies for going back and forth to figure things out (see DeepSeek R1 paper for examples). These strategies would have been very difficult to achieve in the previous paradigms because it's not clear what the optimal reasoning traces and recoveries look like for the LLM - it has to find what works for it, via the optimization against rewards. Unlike the SFT and RLHF stage, which are both relatively thin/short stages (minor finetunes computationally), RLVR involves training against objective (non-gameable) reward functions which allows for a lot longer optimization. Running RLVR turned out to offer high capability/$, which gobbled up the compute that was originally intended for pretraining. Therefore, most of the capability progress of 2025 was defined by the LLM labs chewing through the overhang of this new stage and overall we saw ~similar sized LLMs but a lot longer RL runs. Also unique to this new stage, we got a whole new knob (and and associated scaling law) to control capability as a function of test time compute by generating longer reasoning traces and increasing \"thinking time\". OpenAI o1 (late 2024) was the very first demonstration of an RLVR model, but the o3 release (early 2025) was the obvious point of inflection where you could intuitively feel the difference. 2. Ghosts vs. Animals / Jagged Intelligence2025 is where I (and I think the rest of the industry also) first started to internalize the \"shape\" of LLM intelligence in a more intuitive sense. We're not \"evolving/growing animals\", we are \"summoning ghosts\". Everything about the LLM stack is different (neural architecture, training data, training algorithms, and especially optimization pressure) so it should be no surprise that we are getting very different entities in the intelligence space, which are inappropriate to think about through an animal lens. Supervision bits-wise, human neural nets are optimized for survival of a tribe in the jungle but LLM neural nets are optimized for imitating humanity's text, collecting rewards in math puzzles, and getting that upvote from a human on the LM Arena. As verifiable domains allow for RLVR, LLMs \"spike\" in capability in the vicinity of these domains and overall display amusingly jagged performance characteristics - they are at the same time a genius polymath and a confused and cognitively challenged grade schooler, seconds away from getting tricked by a jailbreak to exfiltrate your data. (human intelligence: blue, AI intelligence: red. I like this version of the meme (I'm sorry I lost the reference to its original post on X) for pointing out that human intelligence is also jagged in its own different way.) Related to all this is my general apathy and loss of trust in benchmarks in 2025. The core issue is that benchmarks are almost by construction verifiable environments and are therefore immediately susceptible to RLVR and weaker forms of it via synthetic data generation. In the typical benchmaxxing process, teams in LLM labs inevitably construct environments adjacent to little pockets of the embedding space occupied by benchmarks and grow jaggies to cover them. Training on the test set is a new art form. What does it look like to crush all the benchmarks but still not get AGI? I have written a lot more on the topic of this section here: Animals vs. Ghosts Verifiability The Space of Minds 3. Cursor / new layer of LLM appsWhat I find most notable about Cursor (other than its meteoric rise this year) is that it convincingly revealed a new layer of an \"LLM app\" - people started to talk about \"Cursor for X\". As I highlighted in my Y Combinator talk this year (transcript and video), LLM apps like Cursor bundle and orchestrate LLM calls for specific verticals: They do the \"context engineering\" They orchestrate multiple LLM calls under the hood strung into increasingly more complex DAGs, carefully balancing performance and cost tradeoffs. They provide an application-specific GUI for the human in the loop They offer an \"autonomy slider\" A lot of chatter has been spent in 2025 on how \"thick\" this new app layer is. Will the LLM labs capture all applications or are there green pastures for LLM apps? Personally I suspect that LLM labs will trend to graduate the generally capable college student, but LLM apps will organize, finetune and actually animate teams of them into deployed professionals in specific verticals by supplying private data, sensors and actuators and feedback loops. 4. Claude Code / AI that lives on your computerClaude Code (CC) emerged as the first convincing demonstration of what an LLM Agent looks like - something that in a loopy way strings together tool use and reasoning for extended problem solving. In addition, CC is notable to me in that it runs on your computer and with your private environment, data and context. I think OpenAI got this wrong because they focused their early codex / agent efforts on cloud deployments in containers orchestrated from ChatGPT instead of simply localhost. And while agent swarms running in the cloud feels like the \"AGI endgame\", we live in an intermediate and slow enough takeoff world of jagged capabilities that it makes more sense to run the agents directly on the developer's computer. Note that the primary distinction that matters is not about where the \"AI ops\" happen to run (in the cloud, locally or whatever), but about everything else - the already-existing and booted up computer, its installation, context, data, secrets, configuration, and the low-latency interaction. Anthropic got this order of precedence correct and packaged CC into a delightful, minimal CLI form factor that changed what AI looks like - it's not just a website you go to like Google, it's a little spirit/ghost that \"lives\" on your computer. This is a new, distinct paradigm of interaction with an AI. 5. Vibe coding2025 is the year that AI crossed a capability threshold necessary to build all kinds of impressive programs simply via English, forgetting that the code even exists. Amusingly, I coined the term \"vibe coding\" in this shower of thoughts tweet totally oblivious to how far it would go :). With vibe coding, programming is not strictly reserved for highly trained professionals, it is something anyone can do. In this capacity, it is yet another example of what I wrote about in Power to the people: How LLMs flip the script on technology diffusion, on how (in sharp contrast to all other technology so far) regular people benefit a lot more from LLMs compared to professionals, corporations and governments. But not only does vibe coding empower regular people to approach programming, it empowers trained professionals to write a lot more (vibe coded) software that would otherwise never be written. In nanochat, I vibe coded my own custom highly efficient BPE tokenizer in Rust instead of having to adopt existing libraries or learn Rust at that level. I vibe coded many projects this ye...",
      "original_url": "https://karpathy.bearblog.dev/year-in-review-2025/"
    },
    {
      "slug": "microgpt-6d759dd0",
      "title": "microgpt",
      "source": "karpathy-github",
      "category": "General",
      "summary": "This is a brief guide to my new art project microgpt, a single file of 200 lines of pure Python with no dependencies that trains and inferences a GPT. This file contains the full algorithmic content of what is needed: dataset of documents, tokenizer, autograd engine, a GPT-2-like neural network architecture, the Adam optimizer, training loop, and inference loop. This script is the culmination of multiple projects (micrograd, makemore, nanogpt, etc.) and a decade-long obsession to simplify LLMs to their bare essentials, and I think it is beautiful ü•π.",
      "content": "This is a brief guide to my new art project microgpt, a single file of 200 lines of pure Python with no dependencies that trains and inferences a GPT. This file contains the full algorithmic content of what is needed: dataset of documents, tokenizer, autograd engine, a GPT-2-like neural network architecture, the Adam optimizer, training loop, and inference loop. Everything else is just efficiency. I cannot simplify this any further. This script is the culmination of multiple projects (micrograd, makemore, nanogpt, etc.) and a decade-long obsession to simplify LLMs to their bare essentials, and I think it is beautiful ü•π. It even breaks perfectly across 3 columns: Where to find it: This GitHub gist has the full source code: microgpt.py It‚Äôs also available on this web page: https://karpathy.ai/microgpt.html Also available as a Google Colab notebook The following is my guide on stepping an interested reader through the code. Dataset The fuel of large language models is a stream of text data, optionally separated into a set of documents. In production-grade applications, each document would be an internet web page but for microgpt we use a simpler example of 32,000 names, one per line: # Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names) if not os.path.exists('input.txt'): import urllib.request names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt' urllib.request.urlretrieve(names_url, 'input.txt') docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents random.shuffle(docs) print(f\"num docs: {len(docs)}\") The dataset looks like this. Each name is a document: emma olivia ava isabella sophia charlotte mia amelia harper ... (~32,000 names follow) The goal of the model is to learn the patterns in the data and then generate similar new documents that share the statistical patterns within. As a preview, by the end of the script our model will generate (‚Äúhallucinate‚Äù!) new, plausible-sounding names. Skipping ahead, we‚Äôll get: sample 1: kamon sample 2: ann sample 3: karai sample 4: jaire sample 5: vialan sample 6: karia sample 7: yeran sample 8: anna sample 9: areli sample 10: kaina sample 11: konna sample 12: keylen sample 13: liole sample 14: alerin sample 15: earan sample 16: lenne sample 17: kana sample 18: lara sample 19: alela sample 20: anton It doesn‚Äôt look like much, but from the perspective of a model like ChatGPT, your conversation with it is just a funny looking ‚Äúdocument‚Äù. When you initialize the document with your prompt, the model‚Äôs response from its perspective is just a statistical document completion. Tokenizer Under the hood, neural networks work with numbers, not characters, so we need a way to convert text into a sequence of integer token ids and back. Production tokenizers like tiktoken (used by GPT-4) operate on chunks of characters for efficiency, but the simplest possible tokenizer just assigns one integer to each unique character in the dataset: # Let there be a Tokenizer to translate strings to discrete symbols and back uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1 BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS print(f\"vocab size: {vocab_size}\") In the code above, we collect all unique characters across the dataset (which are just all the lowercase letters a-z), sort them, and each letter gets an id by its index. Note that the integer values themselves have no meaning at all; each token is just a separate discrete symbol. Instead of 0, 1, 2 they might as well be different emoji. In addition, we create one more special token called BOS (Beginning of Sequence), which acts as a delimiter: it tells the model ‚Äúa new document starts/ends here‚Äù. Later during training, each document gets wrapped with BOS on both sides: [BOS, e, m, m, a, BOS]. The model learns that BOS initates a new name, and that another BOS ends it. Therefore, we have a final vocavulary of 27 (26 possible lowercase characters a-z and +1 for the BOS token). Autograd Training a neural network requires gradients: for each parameter in the model, we need to know ‚Äúif I nudge this number up a little, does the loss go up or down, and by how much?‚Äù. The computation graph has many inputs (the model parameters and the input tokens) but funnels down to a single scalar output: the loss (we‚Äôll define exactly what the loss is below). Backpropagation starts at that single output and works backwards through the graph, computing the gradient of the loss with respect to every input. It relies on the chain rule from calculus. In production, libraries like PyTorch handle this automatically. Here, we implement it from scratch in a single class called Value: class Value: __slots__ = ('data', 'grad', '_children', '_local_grads') def __init__(self, data, children=(), local_grads=()): self.data = data # scalar value of this node calculated during forward pass self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass self._children = children # children of this node in the computation graph self._local_grads = local_grads # local derivative of this node w.r.t. its children def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, (self, other), (1, 1)) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, (self, other), (other.data, self.data)) def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),)) def log(self): return Value(math.log(self.data), (self,), (1/self.data,)) def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),)) def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),)) def __neg__(self): return self * -1 def __radd__(self, other): return self + other def __sub__(self, other): return self + (-other) def __rsub__(self, other): return other + (-self) def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __rtruediv__(self, other): return other * self**-1 def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._children: build_topo(child) topo.append(v) build_topo(self) self.grad = 1 for v in reversed(topo): for child, local_grad in zip(v._children, v._local_grads): child.grad += local_grad * v.grad I realize that this is the most mathematically and algorithmically intense part and I have a 2.5 hour video on it: micrograd video. Briefly, a Value wraps a single scalar number (.data) and tracks how it was computed. Think of each operation as a little lego block: it takes some inputs, produces an output (the forward pass), and it knows how its output would change with respect to each of its inputs (the local gradient). That‚Äôs all the information autograd needs from each block. Everything else is just the chain rule, stringing the blocks together. Every time you do math with Value objects (add, multiply, etc.), the result is a new Value that remembers its inputs (_children) and the local derivative of that operation (_local_grads). For example, __mul__ records that \\(\\frac{\\partial(a \\cdot b)}{\\partial a} = b\\) and \\(\\frac{\\partial(a \\cdot b)}{\\partial b} = a\\). The full set of lego blocks: Operation Forward Local gradients a + b \\(a + b\\) \\(\\frac{\\partial}{\\partial a} = 1, \\quad \\frac{\\partial}{\\partial b} = 1\\) a * b \\(a \\cdot b\\) \\(\\frac{\\partial}{\\partial a} = b, \\quad \\frac{\\partial}{\\partial b} = a\\) a ** n \\(a^n\\) \\(\\frac{\\partial}{\\partial a} = n \\cdot a^{n-1}\\) log(a) \\(\\ln(a)\\) \\(\\frac{\\partial}{\\partial a} = \\frac{1}{a}\\) exp(a) \\(e^a\\) \\(\\frac{\\partial}{\\partial a} = e^a\\) relu(a) \\(\\max(0, a)\\) \\(\\frac{\\partial}{\\partial a} = \\mathbf{1}_{a >...",
      "original_url": "http://karpathy.github.io/2026/02/12/microgpt/"
    },
    {
      "slug": "the-ai-vampire-8ba7700d",
      "title": "The AI Vampire",
      "source": "simonwillison",
      "category": "General",
      "summary": "Simon Willison‚Äôs Weblog Subscribe Sponsored by: Example Sponsor ‚Äî This is a placeholder for a sponsored message. Learn more The AI Vampire (via) Steve Yegge's take on agent fatigue, and its relationship to burnout. Let's pretend you're the only person at your company using AI.",
      "content": "Simon Willison‚Äôs Weblog Subscribe Sponsored by: Example Sponsor ‚Äî This is a placeholder for a sponsored message. Learn more The AI Vampire (via) Steve Yegge's take on agent fatigue, and its relationship to burnout. Let's pretend you're the only person at your company using AI. In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison. In that scenario, your employer captures 100% of the value from you adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now. And you're exhausted. You're tired, Boss. You got nothing for it. Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier. Steve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace: I‚Äôve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice. Posted 15th February 2026 at 11:59 pm Recent articles Two new Showboat tools: Chartroom and datasette-showboat - 17th February 2026 Deep Blue - 15th February 2026 The evolution of OpenAI's mission statement - 13th February 2026 steve-yegge 10 ai 1849 generative-ai 1636 llms 1601 ai-assisted-programming 336 ai-ethics 267 coding-agents 150 cognitive-debt 7 Monthly briefing Sponsor me for $10/month and get a curated email digest of the month's most important LLM developments. Pay me to send you less! Sponsor & subscribe Disclosures Colophon ¬© 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026",
      "original_url": "https://simonwillison.net/2026/Feb/15/the-ai-vampire/#atom-everything"
    },
    {
      "slug": "testing-reachy-mini-hugging-faces-pi-powered-robot-af3666d1",
      "title": "Testing Reachy Mini - Hugging Face's Pi powered robot",
      "source": "jeffgeerling",
      "category": "General",
      "summary": "Testing Reachy Mini - Hugging Face's Pi powered robotFeb 13, 2026When I saw Jensen Huang introduce the Reachy Mini at CES, I thought it was a gimmick. His keynote showed this little robot responding to human input, turning its head to look at a TODO list on the wall, sending emails, and turning drawings into architectural renderings with motion.HuggingFace and Pollen robotics sent me a Reachy Mini to test, and, well, at least if you're looking to replicate that setup in the keynote, it's not, as Jensen put it, \"utterly trivial now.\"On the promise of that keynote, I accepted a review unit, and promptly put it together, with some assistance from my kids. The older kids were interested throughout, but my youngest (1-year-old) was more interested in slamming various parts into the table to make loud sounds, so he had to sit out most of the build.I decided to open up the Conversation App (which connects Reachy Mini to Open AI for real-time interactive chat), and see what would happen if I let my kids start talking.What happened next alarmed me.Within seconds of starting the conversation app, my daughter told the robot her name.",
      "content": "Testing Reachy Mini - Hugging Face's Pi powered robotFeb 13, 2026When I saw Jensen Huang introduce the Reachy Mini at CES, I thought it was a gimmick. His keynote showed this little robot responding to human input, turning its head to look at a TODO list on the wall, sending emails, and turning drawings into architectural renderings with motion.HuggingFace and Pollen robotics sent me a Reachy Mini to test, and, well, at least if you're looking to replicate that setup in the keynote, it's not, as Jensen put it, \"utterly trivial now.\"On the promise of that keynote, I accepted a review unit, and promptly put it together, with some assistance from my kids. The older kids were interested throughout, but my youngest (1-year-old) was more interested in slamming various parts into the table to make loud sounds, so he had to sit out most of the build.I decided to open up the Conversation App (which connects Reachy Mini to Open AI for real-time interactive chat), and see what would happen if I let my kids start talking.What happened next alarmed me.Within seconds of starting the conversation app, my daughter told the robot her name. Before the first couple minutes were up, she started providing the names of her siblings and pointing them out so Reachy could aim the camera around and learn who 'he' was talking to.(The kids quickly anthropomorphised Reachy Mini...). Your browser does not support the video tag.It was not going well, so I quickly stopped the app, under the fear my kids would provide Sam Altman's privacy-invading company with even more data on my family than it has already scraped from every posting ever made on the Internet.I explained how they should treat robots and conversational AI as they would a stranger, and not be so forthcoming.When I turned on the app again, they quickly set out to confuse the bot about who was talking to it. Better, but I'd still give my kids at most a few minutes into a future robot apocalypse before they, too, would turn into human batteries.This blog post is a companion to today's video on my YouTube channel, which you can watch below:Reachy Mini WirelessFor the privacy-conscious, Reachy Mini does not require OpenAI. In fact, the $449 Wireless version I'm testing runs off a Raspberry Pi CM4, so you could flash your own OS and controls to it if you want (or modify any of the reachy-mini source).The intention isn't to replace human interaction, rather it's to inspire learning.To help with the safety aspects, at least physically, Reachy Mini is small and somewhat weak. It shouldn't be able to cause physical harm like Blinky‚Ñ¢ could.The first time I set up Reachy Mini at home, I did have some trouble‚Äîeventually I diagnosed the problem as an IPv6 DNS resolution error.I do have concerns about the 'open by default' nature of the little robot's APIs and web UI, and especially over apps like the conversation app immediately connecting to OpenAI servers without any form of consent. It could quickly become an attack vector if it's not locked down in any way.But I think those issues can be ironed out, along with problems like most of the documentation links on Seeed Studio's Getting Started page linking to pages that don't exist.BuildThe robot only comes in kit form, so you have to put it together. All the plastic parts are made of molded ABS, and were sturdy and fit together well.They included a HuiJiaQi screwdriver that's honestly nicer than some of the ones I use at my workbench, so that was a pleasant surprise.The assembly process is well-documented, and I enlisted the help of my kids to build it. The whole process took less than two hours‚Äîdespite my 1-year-old's attempted destructive testing at the start :)The 'eyes' are just 16mm C-mount fisheye lenses, snapped into some other pieces of convex glass, to create the illusion of depth, √† la Wall-E's eyes. The actual 'eye' is a Raspberry Pi Camera Module 2, mounted in the middle of the face.Someone actually built an ESP32 project to light up the eye lenses, which is pretty cool, but I'd watch out if Reachy's eyes start turning red!QuirksAs I mentioned earlier, the first time I tried booting it up, I was able to get Reachy to wake up through the Reachy Mini Control app on my Mac, but nothing else worked after that. Eventually I found out I had to open up Reachy to the Internet with DNS working over IPv4.I don't like things requiring Internet access, especially if they have cameras, microphones, and are targeted at kids. But, the Seeed Studios privacy section states:Reachy Mini does not send any data to Pollen Robotics or Hugging Face. All processing happens locally unless you explicitly configure cloud services.So the Internet connection snafu I ran into is hopefully just a bug.I could still get to it over SSH without the Internet connection, so I'm still in control even without the app. That's the nice thing about it being open source: I can choose how I use it, and even flash my own OS to the Pi if I wanted.I also had some trouble with controls and some of the apps, depending on the computer and network I was on‚Äîmy Framework laptop running Firefox didn't seem to work with all the apps, and I couldn't get a live camera feed back to it running the Control app. But my Dell GB10 box did work, even though both were running Ubuntu 25.04 and the latest version of Firefox. Go figure!The point is, don't expect this robot to run like Jenson said out of the box (\"utterly trivial\"). This is a robot built for learning, and it is not an appliance you can plug in and get instant agentic AI gratification.Setup and ControlI was pleased to find a full Web API running, so I could hit the robot (at reachy-mini.local:8000) from anywhere on my network and run commands‚Äîthough it would be nice to have at least minimal security built-in, like HTTP basic authentication.You can use the Web API, the Web UI (if you have a Wireless version), the Control app on Mac, Windows or Linux, and there's even a full Python SDK.I wanted to test the Desktop App on my Pi laptop (so I could have a Pi, controlling a Pi, controlling a robot), but they don't have an Arm Linux build yet. The desktop app runs pretty well on macOS, but I found it inconsistent on Linux.MarionetteOne of the easiest ways to demonstrate the hands-on nature of Reachy Mini is an app you can install (via one click in the UI) called 'Marionette'.Riley from LTT tested Marionette over on ShortCircuit to humorous effect, but in his brief time with it, didn't realize you had to open a separate web UI to record motions.You start a recording, physically grab Reachy's head, and start moving it around. The app is supposed to record all that motion, then play it back, but in practice‚Äîat least on my Reachy Mini Wireless, the playback was a bit... off.Other AppsHuggingFace hosts a number of Reachy Mini Apps, from the conversation app and marionette I mentioned earlier, to a metronome that ticks to the beat of the antenna, a hand-tracking app (pictured below), a 'radio' you can tune in by twiddling the right antenna, and more.These demos are good fun, and stepping stones towards building your own customized apps. But they're nowhere near what Nvidia highlighted in the keynote (again, \"utterly trivial\"). They do show some promise; and HuggingFace has a blog post showing how they built their agentic demo‚Äîbut it is a little beyond what I'd call \"trivial\". Especially if you want to run everything local.The end result (a smart speaker that can emote, basically) isn't worth $449, but the learning experience and the ability to have it fully under my own control may be.Having the expressiveness of Reachy Mini is a step up from a basic smart speaker, in terms of interaction. My kids enjoy messing with Siri or Alexa, when they get a chance... but at certain points, it seemed they were talking with Reachy Mini, like they would another human.There's a lot to be said about quick interaction loops and the ability to meld together mics and cameras in real-time.But that requi...",
      "original_url": "https://www.jeffgeerling.com/blog/2026/testing-reachy-mini-hugging-face-robot/"
    },
    {
      "slug": "two-different-tricks-for-fast-llm-inference-9f52f6cb",
      "title": "Two different tricks for fast LLM inference",
      "source": "seangoedecke",
      "category": "General",
      "summary": "Two different tricks for fast LLM inferenceAnthropic and OpenAI both recently announced ‚Äúfast mode‚Äù: a way to interact with their best coding model at significantly higher speeds. These two versions of fast mode are very different. Anthropic‚Äôs offers up to 2.5x tokens per second (so around 170, up from Opus 4.6‚Äôs 65).",
      "content": "Two different tricks for fast LLM inferenceAnthropic and OpenAI both recently announced ‚Äúfast mode‚Äù: a way to interact with their best coding model at significantly higher speeds. These two versions of fast mode are very different. Anthropic‚Äôs offers up to 2.5x tokens per second (so around 170, up from Opus 4.6‚Äôs 65). OpenAI‚Äôs offers more than 1000 tokens per second (up from GPT-5.3-Codex‚Äôs 65 tokens per second, so 15x). So OpenAI‚Äôs fast mode is six times faster than Anthropic‚Äôs1. However, Anthropic‚Äôs big advantage is that they‚Äôre serving their actual model. When you use their fast mode, you get real Opus 4.6, while when you use OpenAI‚Äôs fast mode you get GPT-5.3-Codex-Spark, not the real GPT-5.3-Codex. Spark is indeed much faster, but is a notably less capable model: good enough for many tasks, but it gets confused and messes up tool calls in ways that vanilla GPT-5.3-Codex would never do. Why the differences? The AI labs aren‚Äôt advertising the details of how their fast modes work, but I‚Äôm pretty confident it‚Äôs something like this: Anthropic‚Äôs fast mode is backed by low-batch-size inference, while OpenAI‚Äôs fast mode is backed by special monster Cerebras chips. Let me unpack that a bit. How Anthropic‚Äôs fast mode works The tradeoff at the heart of AI inference economics is batching, because the main bottleneck is memory. GPUs are very fast, but moving data onto a GPU is not. Every inference operation requires copying all the tokens of the user‚Äôs prompt2 onto the GPU before inference can start. Batching multiple users up thus increases overall throughput at the cost of making users wait for the batch to be full. A good analogy is a bus system. If you had zero batching for passengers - if, whenever someone got on a bus, the bus departed immediately - commutes would be much faster for the people who managed to get on a bus. But obviously overall throughput would be much lower, because people would be waiting at the bus stop for hours until they managed to actually get on one. Anthropic‚Äôs fast mode offering is basically a bus pass that guarantees that the bus immediately leaves as soon as you get on. It‚Äôs six times the cost, because you‚Äôre effectively paying for all the other people who could have got on the bus with you, but it‚Äôs way faster3 because you spend zero time waiting for the bus to leave. edit: I want to thank a reader for emailing me to point out that the ‚Äúwaiting for the bus‚Äù cost is really only paid for the first token, so that won‚Äôt affect streaming latency (just latency per turn or tool call). It‚Äôs thus better to think of the performance impact of batch size being mainly that smaller batches require fewer flops and thus execute more quickly. In my analogy, maybe it‚Äôs ‚Äúlighter buses drive faster‚Äù, or something. Obviously I can‚Äôt be fully certain this is right. Maybe they have access to some new ultra-fast compute that they‚Äôre running this on, or they‚Äôre doing some algorithmic trick nobody else has thought of. But I‚Äôm pretty sure this is it. Brand new compute or algorithmic tricks would likely require changes to the model (see below for OpenAI‚Äôs system), and ‚Äúsix times more expensive for 2.5x faster‚Äù is right in the ballpark for the kind of improvement you‚Äôd expect when switching to a low-batch-size regime. How OpenAI‚Äôs fast mode works OpenAI‚Äôs fast mode does not work anything like this. You can tell that simply because they‚Äôre introducing a new, worse model for it. There would be absolutely no reason to do that if they were simply tweaking batch sizes. Also, they told us in the announcement blog post exactly what‚Äôs backing their fast mode: Cerebras. OpenAI announced their Cerebras partnership a month ago in January. What‚Äôs Cerebras? They build ‚Äúultra low-latency compute‚Äù. What this means in practice is that they build giant chips. A H100 chip (fairly close to the frontier of inference chips) is just over a square inch in size. A Cerebras chip is 70 square inches. You can see from pictures that the Cerebras chip has a grid-and-holes pattern all over it. That‚Äôs because silicon wafers this big are supposed to be broken into dozens of chips. Instead, Cerebras etches a giant chip over the entire thing. The larger the chip, the more internal memory it can have. The idea is to have a chip with SRAM large enough to fit the entire model, so inference can happen entirely in-memory. Typically GPU SRAM is measured in the tens of megabytes. That means that a lot of inference time is spent streaming portions of the model weights from outside of SRAM into the GPU compute4. If you could stream all of that from the (much faster) SRAM, inference would a big speedup: fifteen times faster, as it turns out! So how much internal memory does the latest Cerebras chip have? 44GB. This puts OpenAI in kind of an awkward position. 44GB is enough to fit a small model (~20B params at fp16, ~40B params at int8 quantization), but clearly not enough to fit GPT-5.3-Codex. That‚Äôs why they‚Äôre offering a brand new model, and why the Spark model has a bit of ‚Äúsmall model smell‚Äù to it: it‚Äôs a smaller distil of the much larger GPT-5.3-Codex model5. edit: I was wrong about this - the Codex model is almost certainly larger than this, and doesn‚Äôt need to fit entirely in one chip‚Äôs SRAM (if it did, we‚Äôd be seeing faster speeds). Thanks to the Hacker News commenters for correcting me. But I think there‚Äôs still a good chance that Spark is SRAM-resident (split across a few Cerebras chips) which is what‚Äôs driving the speedup. OpenAI‚Äôs version is much more technically impressive It‚Äôs interesting that the two major labs have two very different approaches to building fast AI inference. If I had to guess at a conspiracy theory, it would go something like this: OpenAI partner with Cerebras in mid-January, obviously to work on putting an OpenAI model on a fast Cerebras chip Anthropic have no similar play available, but they know OpenAI will announce some kind of blazing-fast inference in February, and they want to have something in the news cycle to compete with that Anthropic thus hustles to put together the kind of fast inference they can provide: simply lowering the batch size on their existing inference stack Anthropic (probably) waits until a few days before OpenAI are done with their much more complex Cerebras implementation to announce it, so it looks like OpenAI copied them Obviously OpenAI‚Äôs achievement here is more technically impressive. Getting a model running on Cerebras chips is not trivial, because they‚Äôre so weird. Training a 20B or 40B param distil of GPT-5.3-Codex that is still kind-of-good-enough is not trivial. But I commend Anthropic for finding a sneaky way to get ahead of the announcement that will be largely opaque to non-technical people. It reminds me of OpenAI‚Äôs mid-2025 sneaky introduction of the Responses API to help them conceal their reasoning tokens. Is fast AI inference the next big thing? Seeing the two major labs put out this feature might make you think that fast AI inference is the new major goal they‚Äôre chasing. I don‚Äôt think it is. If my theory above is right, Anthropic don‚Äôt care that much about fast inference, they just didn‚Äôt want to appear behind OpenAI. And OpenAI are mainly just exploring the capabilities of their new Cerebras partnership. It‚Äôs still largely an open question what kind of models can fit on these giant chips, how useful those models will be, and if the economics will make any sense. I personally don‚Äôt find ‚Äúfast, less-capable inference‚Äù particularly useful. I‚Äôve been playing around with it in Codex and I don‚Äôt like it. The usefulness of AI agents is dominated by how few mistakes they make, not by their raw speed. Buying 6x the speed at the cost of 20% more mistakes is a bad bargain, because most of the user‚Äôs time is spent handling mistakes instead of waiting for the model6. However, it‚Äôs certainly possible that fast, less-capable inference becomes a core lower-level primitive in AI systems. Claude Code already uses Haiku for some o...",
      "original_url": "https://seangoedecke.com/fast-llm-inference/"
    },
    {
      "slug": "kimwolf-botnet-lurking-in-corporate-govt-networks-47049a71",
      "title": "Kimwolf Botnet Lurking in Corporate, Govt. Networks",
      "source": "krebsonsecurity",
      "category": "General",
      "summary": "January 20, 2026 15 Comments A new Internet-of-Things (IoT) botnet called Kimwolf has spread to more than 2 million devices, forcing infected systems to participate in massive distributed denial-of-service (DDoS) attacks and to relay other malicious and abusive Internet traffic. Kimwolf‚Äôs ability to scan the local networks of compromised systems for other IoT devices to infect makes it a sobering threat to organizations, and new research reveals Kimwolf is surprisingly prevalent in government and corporate networks. Kimwolf grew rapidly in the waning months of 2025 by tricking various ‚Äúresidential proxy‚Äù services into relaying malicious commands to devices on the local networks of those proxy endpoints.",
      "content": "January 20, 2026 15 Comments A new Internet-of-Things (IoT) botnet called Kimwolf has spread to more than 2 million devices, forcing infected systems to participate in massive distributed denial-of-service (DDoS) attacks and to relay other malicious and abusive Internet traffic. Kimwolf‚Äôs ability to scan the local networks of compromised systems for other IoT devices to infect makes it a sobering threat to organizations, and new research reveals Kimwolf is surprisingly prevalent in government and corporate networks. Image: Shutterstock, @Elzicon. Kimwolf grew rapidly in the waning months of 2025 by tricking various ‚Äúresidential proxy‚Äù services into relaying malicious commands to devices on the local networks of those proxy endpoints. Residential proxies are sold as a way to anonymize and localize one‚Äôs Web traffic to a specific region, and the biggest of these services allow customers to route their Internet activity through devices in virtually any country or city around the globe. The malware that turns one‚Äôs Internet connection into a proxy node is often quietly bundled with various mobile apps and games, and it typically forces the infected device to relay malicious and abusive traffic ‚Äî including ad fraud, account takeover attempts, and mass content-scraping. Kimwolf mainly targeted proxies from IPIDEA, a Chinese service that has millions of proxy endpoints for rent on any given week. The Kimwolf operators discovered they could forward malicious commands to the internal networks of IPIDEA proxy endpoints, and then programmatically scan for and infect other vulnerable devices on each endpoint‚Äôs local network. Most of the systems compromised through Kimwolf‚Äôs local network scanning have been unofficial Android TV streaming boxes. These are typically Android Open Source Project devices ‚Äî not Android TV OS devices or Play Protect certified Android devices ‚Äî and they are generally marketed as a way to watch unlimited (read:pirated) video content from popular subscription streaming services for a one-time fee. However, a great many of these TV boxes ship to consumers with residential proxy software pre-installed. What‚Äôs more, they have no real security or authentication built-in: If you can communicate directly with the TV box, you can also easily compromise it with malware. While IPIDEA and other affected proxy providers recently have taken steps to block threats like Kimwolf from going upstream into their endpoints (reportedly with varying degrees of success), the Kimwolf malware remains on millions of infected devices. A screenshot of IPIDEA‚Äôs proxy service. Kimwolf‚Äôs close association with residential proxy networks and compromised Android TV boxes might suggest we‚Äôd find relatively few infections on corporate networks. However, the security firm Infoblox said a recent review of its customer traffic found nearly 25 percent of them made a query to a Kimwolf-related domain name since October 1, 2025, when the botnet first showed signs of life. Infoblox found the affected customers are based all over the world and in a wide range of industry verticals, from education and healthcare to government and finance. ‚ÄúTo be clear, this suggests that nearly 25% of customers had at least one device that was an endpoint in a residential proxy service targeted by Kimwolf operators,‚Äù Infoblox explained. ‚ÄúSuch a device, maybe a phone or a laptop, was essentially co-opted by the threat actor to probe the local network for vulnerable devices. A query means a scan was made, not that new devices were compromised. Lateral movement would fail if there were no vulnerable devices to be found or if the DNS resolution was blocked.‚Äù Synthient, a startup that tracks proxy services and was the first to disclose on January 2 the unique methods Kimwolf uses to spread, found proxy endpoints from IPIDEA were present in alarming numbers at government and academic institutions worldwide. Synthient said it spied at least 33,000 affected Internet addresses at universities and colleges, and nearly 8,000 IPIDEA proxies within various U.S. and foreign government networks. The top 50 domain names sought out by users of IPIDEA‚Äôs residential proxy service, according to Synthient. In a webinar on January 16, experts at the proxy tracking service Spur profiled Internet addresses associated with IPIDEA and 10 other proxy services that were thought to be vulnerable to Kimwolf‚Äôs tricks. Spur found residential proxies in nearly 300 government owned and operated networks, 318 utility companies, 166 healthcare companies or hospitals, and 141 companies in banking and finance. ‚ÄúI looked at the 298 [government] owned and operated [networks], and so many of them were DoD [U.S. Department of Defense], which is kind of terrifying that DoD has IPIDEA and these other proxy services located inside of it,‚Äù Spur Co-Founder Riley Kilmer said. ‚ÄúI don‚Äôt know how these enterprises have these networks set up. It could be that [infected devices] are segregated on the network, that even if you had local access it doesn‚Äôt really mean much. However, it‚Äôs something to be aware of. If a device goes in, anything that device has access to the proxy would have access to.‚Äù Kilmer said Kimwolf demonstrates how a single residential proxy infection can quickly lead to bigger problems for organizations that are harboring unsecured devices behind their firewalls, noting that proxy services present a potentially simple way for attackers to probe other devices on the local network of a targeted organization. ‚ÄúIf you know you have [proxy] infections that are located in a company, you can chose that [network] to come out of and then locally pivot,‚Äù Kilmer said. ‚ÄúIf you have an idea of where to start or look, now you have a foothold in a company or an enterprise based on just that.‚Äù This is the third story in our series on the Kimwolf botnet. Next week, we‚Äôll shed light on the myriad China-based individuals and companies connected to the Badbox 2.0 botnet, the collective name given to a vast number of Android TV streaming box models that ship with no discernible security or authentication built-in, and with residential proxy malware pre-installed. Further reading: The Kimwolf Botnet is Stalking Your Local Network Who Benefitted from the Aisuru and Kimwolf Botnets? A Broken System Fueling Botnets (Synthient). This entry was posted on Tuesday 20th of January 2026 01:19 PM DDoS-for-Hire Internet of Things (IoT) Latest Warnings The Coming Storm Web Fraud 2.0 BadBox 2.0 Infoblox IPidea Kimwolf residential proxies Riley Kilmer Spur Synthient",
      "original_url": "https://krebsonsecurity.com/2026/01/kimwolf-botnet-lurking-in-corporate-govt-networks/"
    }
  ]
}